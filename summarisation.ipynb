{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "# import wandb\n",
    "from datetime import datetime\n",
    "# from wandb.integration.langchain import WandbTracer\n",
    "from langchain.callbacks import ClearMLCallbackHandler\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.llms.base import LLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.agents import load_tools, initialize_agent, AgentExecutor, BaseSingleActionAgent, AgentType\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "os.chdir(\"/notebooks/learn-langchain\")\n",
    "from langchain_app.models.text_generation_web_ui import (\n",
    "    build_text_generation_web_ui_client_llm,\n",
    ")\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "# from langchain.callbacks import WandbCallbackHandler, StdOutCallbackHandler\n",
    "from langchain.callbacks import StdOutCallbackHandler\n",
    "\n",
    "# Set Wandb API key\n",
    "# os.environ[\"WANDB_API_KEY\"] = \"9ae105f6bbe37ca6eff03ea9c3af7df398713e7e\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CLEARML_WEB_HOST=https://app.clear.ml\n",
    "%env CLEARML_API_HOST=https://api.clear.ml\n",
    "%env CLEARML_FILES_HOST=https://files.clear.ml\n",
    "%env CLEARML_API_ACCESS_KEY=R2Q12RB4XKJN6G14YJC4\n",
    "%env CLEARML_API_SECRET_KEY=hfYxpPgSUUm4dRfiH0HsJCARjQsSWNfVkjPgyel9u2HS7ShYwu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clearml_callback = ClearMLCallbackHandler(\n",
    "    task_type=\"inference\",\n",
    "    project_name=\"langchain_callback_demo\",\n",
    "    task_name=\"llm\",\n",
    "    tags=[\"test\"],\n",
    "    # Change the following parameters based on the amount of detail you want tracked\n",
    "    visualize=True,\n",
    "    complexity_metrics=True,\n",
    "    stream_logs=True\n",
    ")\n",
    "callbacks = [StdOutCallbackHandler(), clearml_callback]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract JSON summarise the article and save results to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session_group = datetime.now().strftime(\"%m.%d.%Y_%H.%M.%S\")\n",
    "# wandb_callback = WandbTracer({\"project\": \"wandb_prompts_quickstart\"}\n",
    "# )\n",
    "# callbacks = [StdOutCallbackHandler(), wandb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "json_string = '''```json\n",
    "{\n",
    "   \"category\":\"Visa Business News\",\n",
    "   \"title\":\"Acceptance\",\n",
    "   \"geo\":\"Germany\",\n",
    "   \"audience\":\"Sales\",\n",
    "   \"publication_date\":\"2023-05-04T00:00:00Z\"\n",
    "}\n",
    "```'''\n",
    "json_object = clean_and_load_json(json_string)\n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of LLM for generating a JSON extract\n",
    "llm_extract_json = build_text_generation_web_ui_client_llm(\n",
    "    parameters={\n",
    "    \"max_new_tokens\": 150,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "}\n",
    ")\n",
    "\n",
    "# Create an instance of LLM for generating a summary of 200 tokens - Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.9,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 5 fields\n",
    "text_extract_json_5 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 3 fields\n",
    "text_extract_json_3 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract text for summary\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_filepath = '/notebooks/files/extracted_data.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if os.path.exists(csv_filepath):\n",
    "    # If it exists, load it into a DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "else:\n",
    "    # If it doesn't exist, initialize an empty DataFrame with the necessary columns\n",
    "    df = pd.DataFrame(columns=[\n",
    "    \"effective-date\", \n",
    "    \"mark-your-calendar-date\", \n",
    "    \"article-id\", \n",
    "    \"category\", \n",
    "    \"title\", \n",
    "    \"geo\", \n",
    "    \"audience\", \n",
    "    \"publication_date\",\n",
    "    \"medium_summary\"\n",
    "    ])\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Define a function to sanitize a file name\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "\n",
    "# Get the first PDF file path from the list\n",
    "for file_path in pdf_files:\n",
    "\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 3 fields\n",
    "    \n",
    "    template_json_3 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input:\n",
    "    1. Article ID\n",
    "    2. Effective date\n",
    "    3. Mark your calendar date\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output3_1} the object {output3_2} is a well-formatted instance of the schema.\n",
    "    The object {output3_3} is not well-formatted.\n",
    "    Here is the output schema: {output3_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    As an example, this text:\n",
    "\n",
    "    As published in the DCC Guide—DCC Program Requirements (a Visa Supplemental Requirement; see Additional Resources), acquirers registered in the DCC Compliance Program must register their DCC-enabled merchants with Visa annually. Merchant registration information for the period ending 31 July 2023 is due 15 August 2023. This requirement applies to merchants offering DCC in either a card-present or card-not-present environment. \n",
    "    Article ID: AI13088\n",
    "    Also effective 8 June 2023 edition of the Visa Business News, Visa announced updates to the ATM Locator Update System, including an updated ATM locator template,\n",
    "\n",
    "    Mark Your Calendar:\n",
    "    • DeadlineforDCCCompliance Program annual merchant registration (15 August 2023)\n",
    "    • Deadlinetoderegisterasan acquirer from the DCC program prior to fiscal year 2024 billing (30 September 2023)\n",
    "\n",
    "    Acquirers will receive an email confirming receipt of their submission and will only be contacted in the event of a problem with their registration. Further details about the DCC merchant registration process can be found in the DCC Guide—DCC Program Requirements.\n",
    "\n",
    "    Results in the json:\n",
    "        \n",
    "    \"article-id\": \"AI13088\",\n",
    "    \"effective-date\": \"2023-08-15\",\n",
    "    \"mark-your-calendar-date\": \"2023-06-08\"\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response: \n",
    "    Valid JSON document that adheres to the schema instance: ```json\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 5 fields\n",
    "    template_json_5 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input.\n",
    "    1. Category\n",
    "    2. Publication date\n",
    "    3. Article title\n",
    "    4. GEO\n",
    "    5. Article audience\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output5_1} the object {output5_2} is a well-formatted instance of the schema.\n",
    "    The object {output5_3} is not well-formatted.\n",
    "    Here is the output schema: {output5_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    As an example, this text:\n",
    "    Visa Business News\n",
    "    Risk Products 1 June 2023 Card-Not-Present Token Risk Management Tools and Best Practices\n",
    "    Europe | Acquirers, Issuers, Processors Visa Network; Europe Processing\n",
    "    Overview: Visa is reminding clients of the tools and best practices available for managing token provisioning and transaction risk for the four main token use cases in the card-not-present environment.\n",
    "\n",
    "    Results in the json:\n",
    "    \n",
    "    \"category\": \"Risk Products\",\n",
    "    \"publication_date\": \"2023-06-01\",\n",
    "    \"title\": \"Card-Not-Present Token Risk Management Tools and Best Practices\",\n",
    "    \"geo\": \"Europe\",\n",
    "    \"audience\": \"Acquirers, Issuers, Processors\"\n",
    "    \n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    Valid JSON document that adheres to the schema instance: ```json\n",
    "    \"\"\"\n",
    "\n",
    "    template_eval_json_5 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Evaluate the JSON in the Input below.\n",
    "    Remove unnecessary characters or add missing characters to make the JSON valid.\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create sub-templates for the chain\n",
    "    output3_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output3_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output3_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output3_4 = '{\"type\" : \"object\", \"properties\" : {\"article-id\" : {\"type\" : \"string\", \"maxLength\": 7}, \"effective-date\" : {\"type\" : \"string\", \"format\" : \"date\"}, \"mark-your-calendar-date\" : {\"type\" : \"string\", \"format\" : \"date\"}}}'\n",
    "    output5_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output5_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output5_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output5_4 = '{\"type\": \"object\", \"properties\": {\"category\": {\"type\": \"string\"}, \"title\": {\"type\": \"string\"}, \"geo\": {\"type\": \"string\"}, \"audience\": {\"type\": \"string\"}, \"publication_date\": {\"type\": \"string\", \"format\": \"date\"}}}'\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt_json_5 = PromptTemplate(template=template_json_5, input_variables=['question', \n",
    "                                                                              'output5_1', \n",
    "                                                                              'output5_2',\n",
    "                                                                              'output5_3',\n",
    "                                                                              'output5_4'\n",
    "                                                                              ])\n",
    "    prompt_json_3 = PromptTemplate(template=template_json_3, input_variables=['question', \n",
    "                                                                              'output3_1', \n",
    "                                                                              'output3_2',\n",
    "                                                                              'output3_3',\n",
    "                                                                              'output3_4'\n",
    "                                                                              ])\n",
    "    prompt_eval_json_5 = PromptTemplate(template=template_eval_json_5, input_variables=['question'\n",
    "                                                                                ])\n",
    "\n",
    "\n",
    "    # Create a chain for generating a JSON extract\n",
    "    llm_chain_json_5 = LLMChain(llm=llm_extract_json, prompt=prompt_json_5)\n",
    "    llm_chain_json_3 = LLMChain(llm=llm_extract_json, prompt=prompt_json_3)\n",
    "    llm_chain_eval_json_5 = LLMChain(llm=llm_extract_json, prompt=prompt_eval_json_5)\n",
    "\n",
    "    # Extract text for processing with LLM for JSON extract with 5 fields\n",
    "    docs_json_5 = text_extract_json_5.create_documents([text])\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = docs_json_5[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 5 fields and load resulting JSON into a variable\n",
    "    json_5 = llm_chain_json_5.run({'question': question, \n",
    "                                   'output5_1': output5_1, \n",
    "                                   'output5_2': output5_2,\n",
    "                                   'output5_3': output5_3,\n",
    "                                   'output5_4': output5_4\n",
    "                                   })\n",
    "    # json_5 = llm_chain_eval_json_5.run({'question': json_5\n",
    "    #                                    })\n",
    "    \n",
    "\n",
    "    # Extract text for processing with LLM for JSON extract with 3 fields\n",
    "    docs_json_3 = text_extract_json_3.create_documents([text])\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = docs_json_3[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 3 fields and load resulting JSON into a variable\n",
    "    json_3 = llm_chain_json_3.run({'question': question, \n",
    "                                   'output3_1': output3_1, \n",
    "                                   'output3_2': output3_2,\n",
    "                                   'output3_3': output3_3,\n",
    "                                   'output3_4': output3_4\n",
    "                                  })\n",
    "    \n",
    "    # Print the file path\n",
    "    print(file_path)\n",
    "\n",
    "    # Print the results\n",
    "    print(json_5, json_3)\n",
    "\n",
    "    json_3 = clean_and_load_json(json_3)\n",
    "    json_5 = clean_and_load_json(json_5)\n",
    "    # Combine two JSONs into one\n",
    "    combined_json = {**json_3, **json_5}\n",
    "\n",
    "    # Split text into chunks for summary generation\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary and load resulting summary into a variable\n",
    "    med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Check if the summary is None and replace it with an empty string\n",
    "    if med_sum is None:\n",
    "        med_sum = ''\n",
    "\n",
    "    # Convert the summary to a string\n",
    "    med_sum = str(med_sum)\n",
    "\n",
    "    # Create a new row for the DataFrame\n",
    "    new_row = pd.DataFrame.from_records([combined_json])\n",
    "    new_row['medium_summary'] = [med_sum]\n",
    "    new_row['added'] = datetime.now()\n",
    "\n",
    "    # Add a new row to the existing DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('/notebooks/files/extracted_data.csv', index=False)\n",
    "\n",
    "    def rename_file(df, file_path, fields):\n",
    "        # Get the last row of the DataFrame\n",
    "        row = df.iloc[-1]\n",
    "        \n",
    "        # Initialize an empty list for the new file name components\n",
    "        file_name_components = []\n",
    "        \n",
    "        # For each field...\n",
    "        for field in fields:\n",
    "            # If the field value is not NaN...\n",
    "            if not pd.isna(row[field]):\n",
    "                # Clean the field value and append it to the file name components\n",
    "                file_name_components.append(sanitize_filename(str(row[field])))\n",
    "\n",
    "        # If there are no valid file name components, use the original file name\n",
    "        if not file_name_components:\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            file_name_components.append(base_name)\n",
    "                \n",
    "        # Join the file name components with a dash and add the file extension\n",
    "        new_file_name = '-'.join(file_name_components) + '._pdf'\n",
    "    \n",
    "        # Rename the file\n",
    "        os.rename(file_path, os.path.join(os.path.dirname(file_path), new_file_name))\n",
    "\n",
    "    # Call the function to rename the file at the end of processing each file\n",
    "    rename_file(df, file_path, ['article-id', 'title'])\n",
    "\n",
    "    # Print the new row of a DataFrame\n",
    "    print(new_row)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different parameters of the model for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/test/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Create an instance of LLM for generating a summary based on a 200 token text Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.18,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter for summary generation\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "# Create an empty list to save the results\n",
    "results = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Load PDF file\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #  1. GENERATE MEDIUM SIZE SUMMARY\n",
    "\n",
    "    # Split text into chunks of 2500 characters\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Save results to list\n",
    "    results.append(docs_med_sum)\n",
    "\n",
    "# Print all results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Суммаризация PDF файлов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Суммаризация текста и сохранение результатов в текстовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!/root/miniconda3/envs/textgen/bin/python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from langchain.text_splitter import NLTKTextSplitter\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "\n",
    "# Определяем путь к папке с PDF-файлами\n",
    "input_directory = \"/notebooks/files/test\"\n",
    "\n",
    "# Получаем список PDF-файлов\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Создаем инстанс LLM для генерации summary на основе текста размером 200 токенов Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 250,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Создаем инстанс разделителя текста на 2500 символов с перекрытием в 0 символов (для генерации summary)\n",
    "text_splitter_med_sum = SpacyTextSplitter( \n",
    "    chunk_size=1000,\n",
    ")\n",
    "\n",
    "# Создаем цепочку для генерации summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Загружаем PDF-файл\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #1. ГЕНЕРАЦИЯ MEDIAN SIZE SUMMARY\n",
    "\n",
    "    #Разрезаем текст на куски по 2500 символов \n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "    # Отправляем документы в обработку для генерации summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Заменяем расширение файла на .txt и сохраняем результат\n",
    "    output_file_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        output_file.write(docs_med_sum)\n",
    "\n",
    "    print(f\"Обработан файл: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берем PDF документ и делаем из него текстовый файл с помощью langchain.document_loaders.PDFMinerLoader\n",
    "file_path = '/notebooks/files/AI12944 - Updates to Fraud and Consumer Dispute Rules.pdf'\n",
    "loader = PDFMinerLoader(file_path)\n",
    "document = loader.load()\n",
    "\n",
    "text = document[0].page_content\n",
    "\n",
    "llm = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.001,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"top_k\": 1,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "    })\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose = True)\n",
    "chain.run(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
