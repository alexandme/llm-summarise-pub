{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import wandb\n",
    "from datetime import datetime\n",
    "from wandb.integration.langchain import WandbTracer\n",
    "from langchain.memory import ConversationTokenBufferMemory\n",
    "from langchain.agents.tools import Tool\n",
    "from langchain.llms.base import LLM\n",
    "from langchain import PromptTemplate, LLMChain\n",
    "from langchain.agents import load_tools, initialize_agent, AgentExecutor, BaseSingleActionAgent, AgentType\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.document_loaders import PDFMinerLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "os.chdir(\"/notebooks/learn-langchain\")\n",
    "from langchain_app.models.text_generation_web_ui import (\n",
    "    build_text_generation_web_ui_client_llm,\n",
    ")\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.callbacks import WandbCallbackHandler, StdOutCallbackHandler\n",
    "\n",
    "# Set Wandb API key\n",
    "os.environ[\"WANDB_API_KEY\"] = \"9ae105f6bbe37ca6eff03ea9c3af7df398713e7e\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract JSON summarise the article and save results to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_group = datetime.now().strftime(\"%m.%d.%Y_%H.%M.%S\")\n",
    "wandb_callback = WandbTracer({\"project\": \"wandb_prompts_quickstart\"}\n",
    ")\n",
    "callbacks = [StdOutCallbackHandler(), wandb_callback]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "json_string = '''```json\n",
    "{\n",
    "   \"category\":\"Visa Business News\",\n",
    "   \"title\":\"Acceptance\",\n",
    "   \"geo\":\"Germany\",\n",
    "   \"audience\":\"Sales\",\n",
    "   \"publication_date\":\"2023-05-04T00:00:00Z\"\n",
    "}\n",
    "```'''\n",
    "json_object = clean_and_load_json(json_string)\n",
    "print(json_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of LLM for generating a JSON extract\n",
    "llm_extract_json = build_text_generation_web_ui_client_llm(\n",
    "    parameters={\n",
    "    \"max_new_tokens\": 100,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.01,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.2,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "}\n",
    ")\n",
    "\n",
    "# Create an instance of LLM for generating a summary of 200 tokens - Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.01,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 5 fields\n",
    "text_extract_json_5 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=400,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract JSON with 3 fields\n",
    "text_extract_json_3 = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "\n",
    "# Create an instance of a text splitter to extract text for summary\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "# Define the path to the CSV file\n",
    "csv_filepath = '/notebooks/files/extracted_data.csv'\n",
    "\n",
    "# Check if the CSV file exists\n",
    "if os.path.exists(csv_filepath):\n",
    "    # If it exists, load it into a DataFrame\n",
    "    df = pd.read_csv(csv_filepath)\n",
    "else:\n",
    "    # If it doesn't exist, initialize an empty DataFrame with the necessary columns\n",
    "    df = pd.DataFrame(columns=[\n",
    "    \"effective-date\", \n",
    "    \"mark-your-calendar-date\", \n",
    "    \"article-id\", \n",
    "    \"category\", \n",
    "    \"title\", \n",
    "    \"geo\", \n",
    "    \"audience\", \n",
    "    \"publication_date\",\n",
    "    \"medium_summary\"\n",
    "    ])\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Define a function to sanitize a file name\n",
    "def sanitize_filename(filename):\n",
    "    return re.sub(r'[\\\\/*?:\"<>|]', \"_\", filename)\n",
    "\n",
    "# Define a function to clean and load JSON\n",
    "def clean_and_load_json(json_string):\n",
    "    match = re.search(r'{.*}', json_string, re.DOTALL)\n",
    "    if match:\n",
    "        cleaned_json_string = match.group(0)\n",
    "    else:\n",
    "        cleaned_json_string = json_string\n",
    "    try:\n",
    "        return json.loads(cleaned_json_string)\n",
    "    except json.JSONDecodeError as e:\n",
    "        raise ValueError(f\"Couldn't decode the cleaned string as JSON: {cleaned_json_string}\") from e\n",
    "\n",
    "\n",
    "# Get the first PDF file path from the list\n",
    "for file_path in pdf_files:\n",
    "\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 3 fields\n",
    "    \n",
    "    template_json_3 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input:\n",
    "    1. Article ID\n",
    "    2. Effective date\n",
    "    3. Mark your calendar date\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output3_1} the object {output3_2} is a well-formatted instance of the schema.\n",
    "    The object {output3_3} is not well-formatted.\n",
    "    Here is the output schema: {output3_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a template for the prompt extract to JSON with 5 fields\n",
    "    template_json_5 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Extract the following information from the text in input.\n",
    "    1. Category\n",
    "    2. Title\n",
    "    3. GEO\n",
    "    4. Audience\n",
    "    5. Publication date\n",
    "\n",
    "    You must format your output as a JSON value that adheres to a given \"JSON Schema\" instance.\n",
    "    \"JSON Schema\" is a declarative language that allows you to annotate and validate JSON documents.\n",
    "    As an example, for the schema {output5_1} the object {output5_2} is a well-formatted instance of the schema.\n",
    "    The object {output5_3} is not well-formatted.\n",
    "    Here is the output schema: {output5_4}\n",
    "    Your output will be parsed and type-checked according to the provided schema instance, so make sure all fields in your output match exactly!\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    template_eval_json_5 = \"\"\"\n",
    "    Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "    ### Instruction: \n",
    "    Evaluate the JSON in the Input below.\n",
    "    Remove unnecessary characters or add missing characters to make the JSON valid.\n",
    "\n",
    "    ### Input: \n",
    "    {question}\n",
    "\n",
    "    ### Response:\n",
    "    JSON:\n",
    "    \"\"\"\n",
    "\n",
    "    # Create sub-templates for the chain\n",
    "    output3_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output3_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output3_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output3_4 = '{\"type\" : \"object\", \"properties\" : {\"article-id\" : {\"type\" : \"string\"}, {\"effective-date\" : {\"type\" : \"date\"}, {\"mark-your-calendar-date\" : {\"type\" : \"date\"} } } } }'\n",
    "    output5_1 = '{\"properties\": {\"foo\": {\"title\": \"Foo\", \"description\": \"a list of strings\", \"type\": \"array\", \"items\": {\"type\": \"string\"}}}, \"required\": [\"foo\"]}}'\n",
    "    output5_2 = '{\"foo\": [\"bar\", \"baz\"]}'\n",
    "    output5_3 = '{\"properties\": {\"foo\": [\"bar\", \"baz\"]}}'\n",
    "    output5_4 = '{\"type\" : \"object\", \"properties\" : {\"category\" : {\"type\" : \"string\"}, {\"title\" : {\"type\" : \"string\"}, {\"geo\" : {\"type\" : \"string\"}, {\"audience\" : {\"type\" : \"string\"}, {\"publication_date\" : {\"type\" : \"date\"} } } } } } }'\n",
    "\n",
    "    # Create a prompt\n",
    "    prompt_json_5 = PromptTemplate(template=template_json_5, input_variables=['question', \n",
    "                                                                              'output5_1', \n",
    "                                                                              'output5_2',\n",
    "                                                                              'output5_3',\n",
    "                                                                              'output5_4'\n",
    "                                                                              ])\n",
    "    prompt_json_3 = PromptTemplate(template=template_json_3, input_variables=['question', \n",
    "                                                                              'output3_1', \n",
    "                                                                              'output3_2',\n",
    "                                                                              'output3_3',\n",
    "                                                                              'output3_4'\n",
    "                                                                              ])\n",
    "    prompt_eval_json_5 = PromptTemplate(template=template_eval_json_5, input_variables=['question'\n",
    "                                                                                ])\n",
    "\n",
    "\n",
    "    # Create a chain for generating a JSON extract\n",
    "    llm_chain_json_5 = LLMChain(llm=llm_extract_json, prompt=prompt_json_5)\n",
    "    llm_chain_json_3 = LLMChain(llm=llm_extract_json, prompt=prompt_json_3)\n",
    "    llm_chain_eval_json_5 = LLMChain(llm=llm_extract_json, prompt=prompt_eval_json_5)\n",
    "\n",
    "    # Extract text for processing with LLM for JSON extract with 5 fields\n",
    "    docs_json_5 = text_extract_json_5.create_documents([text])\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = docs_json_5[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 5 fields and load resulting JSON into a variable\n",
    "    json_5 = llm_chain_json_5.run({'question': question, \n",
    "                                   'output5_1': output5_1, \n",
    "                                   'output5_2': output5_2,\n",
    "                                   'output5_3': output5_3,\n",
    "                                   'output5_4': output5_4\n",
    "                                   })\n",
    "    json_5 = llm_chain_eval_json_5.run({'question': json_5\n",
    "                                       })\n",
    "    \n",
    "\n",
    "    # Extract text for processing with LLM for JSON extract with 3 fields\n",
    "    docs_json_3 = text_extract_json_3.create_documents([text])\n",
    "\n",
    "    # Get the text into a variable\n",
    "    question = docs_json_3[0].page_content\n",
    "\n",
    "    # Run the chain for JSON extract with 3 fields and load resulting JSON into a variable\n",
    "    json_3 = llm_chain_json_3.run({'question': question, \n",
    "                                   'output3_1': output3_1, \n",
    "                                   'output3_2': output3_2,\n",
    "                                   'output3_3': output3_3,\n",
    "                                   'output3_4': output3_4\n",
    "                                  })\n",
    "    \n",
    "    # Print the file path\n",
    "    print(file_path)\n",
    "\n",
    "    # Print the results\n",
    "    print(json_5, json_3)\n",
    "\n",
    "    json_3 = json.loads(json_3)\n",
    "    json_5 = json.loads(json_5)\n",
    "    # Combine two JSONs into one\n",
    "    combined_json = {**json_3, **json_5}\n",
    "\n",
    "    # Split text into chunks for summary generation\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary and load resulting summary into a variable\n",
    "    med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Check if the summary is None and replace it with an empty string\n",
    "    if med_sum is None:\n",
    "        med_sum = ''\n",
    "\n",
    "    # Convert the summary to a string\n",
    "    med_sum = str(med_sum)\n",
    "\n",
    "    # Create a new row for the DataFrame\n",
    "    new_row = pd.DataFrame.from_records([combined_json])\n",
    "    new_row['medium_summary'] = [med_sum]\n",
    "\n",
    "    # Add a new row to the existing DataFrame\n",
    "    df = pd.concat([df, new_row], ignore_index=True)\n",
    "\n",
    "    # Save the DataFrame to a CSV file\n",
    "    df.to_csv('/notebooks/files/extracted_data.csv', index=False)\n",
    "\n",
    "    def rename_file(df, file_path, fields):\n",
    "        # Get the last row of the DataFrame\n",
    "        row = df.iloc[-1]\n",
    "        \n",
    "        # Initialize an empty list for the new file name components\n",
    "        file_name_components = []\n",
    "        \n",
    "        # For each field...\n",
    "        for field in fields:\n",
    "            # If the field value is not NaN...\n",
    "            if not pd.isna(row[field]):\n",
    "                # Clean the field value and append it to the file name components\n",
    "                file_name_components.append(sanitize_filename(str(row[field])))\n",
    "\n",
    "        # If there are no valid file name components, use the original file name\n",
    "        if not file_name_components:\n",
    "            base_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "            file_name_components.append(base_name)\n",
    "                \n",
    "        # Join the file name components with a dash and add the file extension\n",
    "        new_file_name = '-'.join(file_name_components) + '._pdf'\n",
    "    \n",
    "        # Rename the file\n",
    "        os.rename(file_path, os.path.join(os.path.dirname(file_path), new_file_name))\n",
    "\n",
    "    # Call the function to rename the file at the end of processing each file\n",
    "    rename_file(df, file_path, ['article-id', 'title'])\n",
    "\n",
    "    # Print the new row of a DataFrame\n",
    "    print(new_row)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing different parameters of the model for text summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the folder with PDF files\n",
    "input_directory = \"/notebooks/files/test/\"\n",
    "\n",
    "# Define the path to the folder with PDF files\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Create an instance of LLM for generating a summary based on a 200 token text Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 0.1,\n",
    "    \"top_p\": 0.4,\n",
    "    \"typical_p\": 1,\n",
    "    \"repetition_penalty\": 1.18,\n",
    "    \"top_k\": 40,\n",
    "    \"min_length\": 0,\n",
    "    \"no_repeat_ngram_size\": 0,\n",
    "    \"num_beams\": 1,\n",
    "    \"penalty_alpha\": 0,\n",
    "    \"length_penalty\": 1,\n",
    "    \"early_stopping\": False,\n",
    "    \"seed\": -1,\n",
    "    \"add_bos_token\": True,\n",
    "    \"truncation_length\": 2048,\n",
    "    \"ban_eos_token\": False,\n",
    "    \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Create an instance of a text splitter for summary generation\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Create a chain for generating summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\", verbose=True)\n",
    "\n",
    "# Create an empty list to save the results\n",
    "results = []\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Load PDF file\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #  1. GENERATE MEDIUM SIZE SUMMARY\n",
    "\n",
    "    # Split text into chunks of 2500 characters\n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "\n",
    "    # Send documents for processing to generate summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Save results to list\n",
    "    results.append(docs_med_sum)\n",
    "\n",
    "# Print all results\n",
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Суммаризация PDF файлов"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Суммаризация текста и сохранение результатов в текстовый файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Определяем путь к папке с PDF-файлами\n",
    "input_directory = \"/notebooks/files\"\n",
    "\n",
    "# Получаем список PDF-файлов\n",
    "pdf_files = glob.glob(os.path.join(input_directory, \"*.pdf\"))\n",
    "\n",
    "# Создаем инстанс LLM для генерации summary на основе текста размером 200 токенов Medium Size Summary (med_sum)\n",
    "llm_med_sum = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 300,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.1,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.18,\n",
    "        \"top_k\": 40,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "})\n",
    "\n",
    "# Создаем инстанс разделителя текста на 2500 символов с перекрытием в 0 символов (для генерации summary)\n",
    "text_splitter_med_sum = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1500,\n",
    "    chunk_overlap=100\n",
    ")\n",
    "\n",
    "# Создаем цепочку для генерации summary\n",
    "chain_med_sum = load_summarize_chain(llm_med_sum, chain_type=\"map_reduce\")\n",
    "\n",
    "\n",
    "for file_path in pdf_files:\n",
    "    # Загружаем PDF-файл\n",
    "    loader = PDFMinerLoader(file_path)\n",
    "    document = loader.load()\n",
    "    text = document[0].page_content\n",
    "\n",
    "    #1. ГЕНЕРАЦИЯ MEDIAN SIZE SUMMARY\n",
    "\n",
    "    #Разрезаем текст на куски по 2500 символов \n",
    "    docs = text_splitter_med_sum.create_documents([text])\n",
    "    # Отправляем документы в обработку для генерации summary\n",
    "    docs_med_sum = chain_med_sum.run(docs)\n",
    "\n",
    "    # Заменяем расширение файла на .txt и сохраняем результат\n",
    "    output_file_path = os.path.splitext(file_path)[0] + \".txt\"\n",
    "    with open(output_file_path, \"w\") as output_file:\n",
    "        output_file.write(docs_med_sum)\n",
    "\n",
    "    print(f\"Обработан файл: {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Берем PDF документ и делаем из него текстовый файл с помощью langchain.document_loaders.PDFMinerLoader\n",
    "file_path = '/notebooks/files/AI12944 - Updates to Fraud and Consumer Dispute Rules.pdf'\n",
    "loader = PDFMinerLoader(file_path)\n",
    "document = loader.load()\n",
    "\n",
    "text = document[0].page_content\n",
    "\n",
    "llm = build_text_generation_web_ui_client_llm(parameters={\n",
    "    \"max_new_tokens\": 200,\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.001,\n",
    "        \"top_p\": 0.1,\n",
    "        \"typical_p\": 1,\n",
    "        \"repetition_penalty\": 1.2,\n",
    "        \"top_k\": 1,\n",
    "        \"min_length\": 32,\n",
    "        \"no_repeat_ngram_size\": 0,\n",
    "        \"num_beams\": 1,\n",
    "        \"penalty_alpha\": 0,\n",
    "        \"length_penalty\": 1,\n",
    "        \"early_stopping\": False,\n",
    "        \"seed\": -1,\n",
    "        \"add_bos_token\": True,\n",
    "        \"truncation_length\": 2048,\n",
    "        \"ban_eos_token\": False,\n",
    "        \"skip_special_tokens\": True,\n",
    "    })\n",
    "\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=2500,\n",
    "    chunk_overlap=0\n",
    ")\n",
    "docs = text_splitter.create_documents([text])\n",
    "\n",
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose = True)\n",
    "chain.run(docs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "textgen",
   "language": "python",
   "name": "textgen"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
